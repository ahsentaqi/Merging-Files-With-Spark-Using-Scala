{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading https://repo1.maven.org/maven2/org/slf4j/slf4j-log4j12/1.7.16/slf4j-log4j12-1.7.16.pom.sha1\n",
      "Downloading https://repo1.maven.org/maven2/org/slf4j/slf4j-log4j12/1.7.16/slf4j-log4j12-1.7.16.pom\n",
      "Downloaded https://repo1.maven.org/maven2/org/slf4j/slf4j-log4j12/1.7.16/slf4j-log4j12-1.7.16.pom\n",
      "Downloaded https://repo1.maven.org/maven2/org/slf4j/slf4j-log4j12/1.7.16/slf4j-log4j12-1.7.16.pom.sha1\n",
      "Downloading https://repo1.maven.org/maven2/org/slf4j/slf4j-log4j12/1.7.16/\n",
      "Downloaded https://repo1.maven.org/maven2/org/slf4j/slf4j-log4j12/1.7.16/\n",
      "Downloading https://repo1.maven.org/maven2/org/slf4j/slf4j-log4j12/1.7.16/slf4j-log4j12-1.7.16.jar\n",
      "Downloading https://repo1.maven.org/maven2/org/slf4j/slf4j-log4j12/1.7.16/slf4j-log4j12-1.7.16.jar.sha1\n",
      "Downloaded https://repo1.maven.org/maven2/org/slf4j/slf4j-log4j12/1.7.16/slf4j-log4j12-1.7.16.jar\n",
      "Downloaded https://repo1.maven.org/maven2/org/slf4j/slf4j-log4j12/1.7.16/slf4j-log4j12-1.7.16.jar.sha1\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\u001b[32mimport \u001b[39m\u001b[36m$ivy.$                                  // adjust spark version - spark >= 2.0\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36m$ivy.$                                // for JupyterSparkSession (SparkSession aware of the jupyter-scala kernel)\n",
       "\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36morg.apache.spark._\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36morg.apache.spark.sql._\n",
       "\u001b[39m\n",
       "\u001b[32mimport \u001b[39m\u001b[36mjupyter.spark.session._\u001b[39m"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import $ivy.`org.apache.spark::spark-sql:2.3.0`// adjust spark version - spark >= 2.0\n",
    "import $ivy.`org.jupyter-scala::spark:0.4.2` // for JupyterSparkSession (SparkSession aware of the jupyter-scala kernel)\n",
    "\n",
    "import org.apache.spark._\n",
    "import org.apache.spark.sql._\n",
    "import jupyter.spark.session._\n",
    "\n",
    "\n",
    "// import org.apache.spark.SparkConf\n",
    "// import org.apache.spark.SparkContext\n",
    "// import org.apache.spark.rdd.RDD.rddToPairRDDFunctions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[36mspark\u001b[39m: \u001b[32mSparkSession\u001b[39m = org.apache.spark.sql.SparkSession@3125f0a4"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val conf = new SparkConf()\n",
    "      .setAppName(\"Exercise\")\n",
    "      .setMaster(\"local\")\n",
    "\n",
    "val sc = new SparkContext(conf)\n",
    "val sqlContext = new org.apache.spark.sql.SQLContext(sc)\n",
    "\n",
    "val spark = SparkSession\n",
    "  .builder()\n",
    "  .appName(\"Spark Sql Session\")\n",
    "  .config(\"spark.some.config.option\", \"test\")\n",
    "  .getOrCreate()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\u001b[36mfilepath\u001b[39m: \u001b[32mrdd\u001b[39m.\u001b[32mRDD\u001b[39m[\u001b[32mString\u001b[39m] = MapPartitionsRDD[53] at keys at cmd6.sc:1\n",
       "\u001b[36mlist\u001b[39m: \u001b[32mList\u001b[39m[\u001b[32mString\u001b[39m] = \u001b[33mList\u001b[39m(\n",
       "  \u001b[32m\"file:/home/ahsen/smat/Ahsen_data/Learning Material/DataEngineering Notebooks/Merging-Files-With-Spark-Using-Scala/MergeFiles/3g_File2.txt\"\u001b[39m,\n",
       "  \u001b[32m\"file:/home/ahsen/smat/Ahsen_data/Learning Material/DataEngineering Notebooks/Merging-Files-With-Spark-Using-Scala/MergeFiles/3g_File3.txt\"\u001b[39m,\n",
       "  \u001b[32m\"file:/home/ahsen/smat/Ahsen_data/Learning Material/DataEngineering Notebooks/Merging-Files-With-Spark-Using-Scala/MergeFiles/3g_File4.txt\"\u001b[39m,\n",
       "  \u001b[32m\"file:/home/ahsen/smat/Ahsen_data/Learning Material/DataEngineering Notebooks/Merging-Files-With-Spark-Using-Scala/MergeFiles/3g_File1.txt\"\u001b[39m\n",
       ")\n",
       "\u001b[36mi\u001b[39m: \u001b[32mInt\u001b[39m = \u001b[32m5\u001b[39m\n",
       "\u001b[36mmergedDF\u001b[39m: \u001b[32mDataFrame\u001b[39m = [Col1: string, Col2: string ... 5 more fields]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val filepath = sc.wholeTextFiles(\"path/to/sampleinputfiles/*.txt\").keys\n",
    "val list = filepath.collect().toList\n",
    "var i = 1\n",
    "list.foreach{ path  =>\n",
    "val df = sqlContext.read\n",
    "    .format(\"com.databricks.spark.csv\")\n",
    "    .option(\"delimiter\", \",\")\n",
    "    .option(\"header\", \"true\")\n",
    "    .load(path)\n",
    "df.write.parquet(\"data/test_tbl/key=\"+ i)\n",
    "    i +=1\n",
    "}\n",
    "val mergedDF = spark.read.option(\"mergeSchema\", \"true\").parquet(\"data/test_tbl\")\n",
    "\n",
    "\n",
    "mergedDF.write.format(\"csv\").save(\"mergedFile/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+----+----+----+----+----+---+\n",
      "|Col1|Col2|Col3|Col4|Col6|Col5|key|\n",
      "+----+----+----+----+----+----+---+\n",
      "| aaa|   2|   3|   4|   6|null|  2|\n",
      "| aaa|   2|   3|   4|   6|null|  2|\n",
      "| aaa|   2|   3|   4|   6|null|  2|\n",
      "| aaa|   2|   3|   4|   6|null|  2|\n",
      "| aaa|   2|   3|   4|   6|null|  2|\n",
      "| aaa|   2|   3|   4|   6|null|  2|\n",
      "| aaa|   2|   3|   4|   6|null|  2|\n",
      "| aaa|   2|   3|   4|   6|null|  2|\n",
      "| aaa|   2|   3|   4|   6|null|  2|\n",
      "| aaa|   2|   3|   4|null|   5|  4|\n",
      "| aaa|   2|   3|   4|null|   5|  4|\n",
      "| aaa|   2|   3|   4|null|   5|  4|\n",
      "| aaa|   2|   3|   4|null|   5|  4|\n",
      "| aaa|   2|   3|   4|null|   5|  4|\n",
      "| aaa|   2|   3|   4|null|   5|  4|\n",
      "| aaa|   2|   3|   4|null|   5|  4|\n",
      "| aaa|   2|   3|   4|null|   5|  4|\n",
      "| aaa|   2|   3|   4|null|   5|  4|\n",
      "| aaa|   2|   3|   4|null|   5|  3|\n",
      "| aaa|   2|   3|   4|null|   5|  3|\n",
      "+----+----+----+----+----+----+---+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "mergedDF.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Scala",
   "language": "scala",
   "name": "scala"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".scala",
   "mimetype": "text/x-scala",
   "name": "scala211",
   "nbconvert_exporter": "script",
   "pygments_lexer": "scala",
   "version": "2.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
